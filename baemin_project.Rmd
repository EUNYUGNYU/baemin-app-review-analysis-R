---
title: "baemin_project"
author: "Seo eunyu"
date: '2021 6 14 '
output:
  html_document:
    fig_height: 6
    fig_width: 10
    highlight: textmate
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: yes
---

# 0. 패키지 불러오기

###### 텍스트 마이닝 프로젝트를 진행하기 전 기본적으로 불러와야 하는 패키지를 불러온다. 

###### 주석에 어떤 경우에 사용하는 패키지인지 기재하였다.


```{r}
library(readr) # SentiWord_Dict.txt을 불러오는 패키지
library(readxl) # 엑셀 파일을 불러오는 패키지
library(dplyr) # 데이터 처리시 유용한 패키지
library(stringr) # 전처리시 사용하는 패키지
library(textclean) #전처리(HTML 특수 문자 제거)시 사용하는 패키지
library(tidytext) # 명사, beta, gamma 추출시 사용하는 패키지
library(KoNLP) # 명사 추출시 사용하는 패키지
library(scales) # 그래프 축 맞출 때 사용되는 패키지
library(ggplot2) # 그래프 그릴 때 사용되는 패키지
library(tidyr) #로그오즈비를 구할 때 사용되는 패키지
library(ldatuning) # 최적 lda 모델을 찾을 때 사용하는 패키지
library(tidylo) # 가중 로그 오즈비를 구하는 패키지
library(ggwordcloud) # 워드 클라우드 그리는 패키지
library(showtext)  # 외부 글꼴 추가 패키지
library(gridExtra) # 워드클라우드 동시에 불러오는 패키지
font_add_google(name = "Black Han Sans", family = "blackhansans") # '검은고딕' 폰트 적용
showtext_auto()
```

# 1. 배달의 민족을 읽어 들여 텍스트 분석을 위한 전처리 하는 과정을 보이고 설명하시오. (10)  


###### 주석에 상세 설명이 달려 있다.

```{r}

# 데이터 불러오기
DF <- read_excel("배달의 민족.xlsx") %>% 
  mutate(id = row_number()) # document를 편하게 구분하기 위해 id번호를 생성.

# 데이터 간단 탐색
head(DF,10)
glimpse(DF)
```

### 1) 기본적인 전처리를 진행한다.


###### 중복되거나 (현재는 중복 리뷰 없음.) 짧은 리뷰을 없애준다.

###### 문제에 제시된 것처럼 단어의 길이가 적어도 2 이상인 것만 추출한다. Topic modeling은 짧은 문서는 적합하지 않다.


```{r}
baemin <- DF %>%
  mutate(reply = str_replace_all(리뷰내용, "[^가-힣]", " "),
         reply = str_squish(reply)) %>%  # 닉네임별로 중복되는 리뷰이 없으니 따로 중복 리뷰 제거할 필요 없음
  filter(str_count(reply, pattern=boundary(type= "word")) >= 2)  # 짧은 문서 제거 :  단어의 길이가 적어도 2 이상인 것만 추출 (Topic modeling은 짧은 문서는 적합하지 않음. 문제에서도 최소 글자수 2로 제한됨.)

```


### 2) 토근화를 진행하여 명사를 추출한다. 


##### 토큰화 단위는 extractNoun (형태소 분석된 명사 형태) 이다.

##### 최소 2번 이상 나오는  나오는 것만 사용한다.


```{r}
# 명사 추출 (토큰화) : 리뷰 (원문)내 중복 단어를 제거하지 않음.
comment <- baemin %>%
  unnest_tokens(input = reply, 
                output = word,
                token = extractNoun,
                drop = F) %>% 
  filter(str_count(word) > 1) # 두 글자 이상 추출

comment<-comment %>%
  select(id, word) %>% print()
```

### 3) 리뷰의 주제어, 즉 모든 리뷰에 빈도 높은 단어 제거한다.

###### 여기서는 "배민", "배달의민족"이다. 제거해준다.

```{r}

# 리뷰의 주제어 "배민" "배달의민족" 삭제
count_word <- comment %>%
  add_count(word, sort=TRUE) %>% # mutate성의 개별 결과 제공
  filter(!word %in% c("배민","배달의민족")) %>% print() 
```

###### 그런데 고객이 "배달의 민족" 이라고 리뷰에 기재한 경우에는 명사 단위로 추출된 상황이기 때문에 "배달"과 "민족" 의 단어로 구분된다. 

###### 이 경우 "배달" 혹은 "민족" 역시 제거해야 할까? 


```{r}
# "배달" 이 전체 리뷰에 몇번 들어갔는지 확인하기
count_word %>% 
  count(word) %>% 
  filter(word=="배달")

# "민족" 이 전체 리뷰에 몇번 들어갔는지 확인하기
count_word %>% 
  count(word) %>% 
  filter(word=="민족")
```

###### "배달" 이라는 단어는 꼭 "배달의 민족" 이라는 문장에서만 사용되는 단어가 아니다. 

###### "배달 서비스", "배달 라이더" 등 다양한 경우에 사용된다. 

###### 반면, 민족은 해당 원문에서는 "배달의 민족" 외에 다른 문장에 비교적사용될 가능성이 적다. <br/><br/>


###### 따라서 해당 두 단어를 삭제해야하는지 판단하기 위해 "배달"과 "민족" 각 몇번씩 사용되었는지를 확인하였다. 

###### 확인 결과 "배달"이라는 단어는 500번 이상 (573번) 사용되었지만 "민족"은 50번 정도 (54번) 만 사용된 것으로 확인되었다.

###### 즉, "배달" 과 "민족" 이 항상 상호명으로 같이 쓰이는 것은 아니라는 것이 확인되었다.

###### 또한, "배반의 민족", "배달의 게르만족" 등 "배달의 민족" 이라는 상호가 변형되서 쓰이는 경우도 많은 것으로 확인되었다.  <br/><br/>



###### 따라서 "배달"과 "민족"은 제거하지 않았으며, "배민"과 "배달의민족"만 제거하였다.



### 4) 불용어 및 유의어 처리

###### stopword (불용어) 처리를 위해 View 함수 사용해서 많이 등장한 (기준: n) 상위 200개의 단어를 확인하였다. 

###### 상위 200개의 단어들 중 stopword 를 찾아 제거하였다.<br/><br/>


###### 유의어로는 업뎃" = "업데이트", "독점" = "독과점"을 확인하였다. 

###### "독점"과 "독과점"은 사전에 정의된 정확한 뜻은 다르지만, 원문에서는 유사한 의미로 쓰이므로 "독점"을 "독과점"으로 수정하였다.



```{r}

count_word %>%
  count(word, sort=TRUE) %>% View() # stopword 처리를 위해 View 함수 사용

# stopword 제거 (기준 : 상위 200개 단어)
stopword <- c("들이", "하다", "하게", "하면", "해서", "이번", "하네",
              "해요", "이것", "니들", "하기", "하지", "한거", "해주",
              "그것", "어디", "여기", "까지", "이거", "하신", "만큼",
              "진짜", "하려", "하시","하라", "한거", "해도", "그거", "그걸")

count_word <- count_word %>%
  filter(!word %in% stopword) %>%
  # 유의어 처리
  mutate(word = dplyr::recode(word,
                              "업뎃" = "업데이트",
                              "독점" = "독과점")) # 해당 원문에서는 독점과 독과점이 거의 유사한 뜻으로 사용됨

```


### 5) 이외의 데이터 처리 (혹은 전처리) 는 분석 기법에 맞게 진행해야 한다. 

###### 따라서, 각 문제를 확인한 후, 프로젝트를 전개하며 진행해준다. <br/><br/>




# 2. 전체 리뷰들에 대한 워드클라우드를 그리고 설명을 하시오. (10)


##### 워드클라우드란? 

##### : 핵심단어를 시각화하는 기법이다. 즉, 키워드, 개념 등을 직관적으로 파악할 수 있도록 많이 언급된 핵심 단어를 시각적으로 돋보이게 하는 기법이다.


###### 해당 프로젝트에서 표현된 워드 클라우드는 해당 단어가 등장한 수를 (빈도, n) 기준으로 하여 워드 클라우드를 이루는 텍스트의 크기를 구분하였다. 

###### 가장 크기가 큰 단어는 가장 많이 사용된 단어이다.

###### 최소 30번 이상 사용된 단어로만 구성되어 있다. 

###### 또한, 빈도에 따라 글자색도 다르게 표현하였다.


```{r}

# 단어별 사용 빈도 (add_count에서 반복 제거해준 것)
word_cloud <-count_word %>% 
  count(word, sort=T) %>% print()

# 워드 클라우드 그리기
word_cloud %>% 
  ggplot(aes(label = word, size = n, 
             color = factor(sample.int(n=10, 
                                       size=nrow(word_cloud), replace = TRUE)))) + # 색깔 지정
  geom_text_wordcloud(seed = 1234, family = "blackhansans") + # 폰트 적용
  scale_radius(limits = c(30, NA), # 최소, 최대 단어 빈도 (최소 30번 이상 나온 단어들이 나온다.)
               range = c(8, 60))+  # 최소, 최대 글자 크기
  labs(title = "전체 리뷰의 Word Cloud") +
  theme(plot.title = element_text(size = 50, face = "bold"))+
  theme_minimal() 

```


#### => 워드 클라우드 만든 후 확인한 결과, 전체 리뷰에서 가장 많이 사용된 단어는 '배달'이고 그 다음으로 '주문', '수수료' 순으로 많이 사용된 것이 나타난다. 

#### 그 외에도 '어플', '사용', '리뷰', '업데이트' 등 주로 배달의 민족 서비스가 운영되는 플랫폼인 '어플리케이션'과 관련된 단어가 주로 등장하는 것이 확인된다.

#### 워드 클라우드를 통해 해당 원문, 고객 리뷰가 전체적으로 어떤 내용에 대해 언급하고 있는지 대략적인 파악이 가능하다. <br/><br/><br/>




# 3. 토픽을 찾기 위한 적합한 토픽의 수를 찾고 설명하기 바랍니다. (10)



##### 적합한 토픽의 수를 찾는 방법을 하이퍼 파라미터 튜닝, 초모수 튜닝이라고 한다.


###### 초모수 튜닝은 토픽 수를 바꾸어 가며 여러 모델을 만든 후 모델별 성능을 비교한 것이다.


###### 토픽 수가 너무 적으면 대부분의 단어가 한 토픽의 주요 단어가 되므로, 토픽 모델링을 하지 않고 그냥 원 데이터 그 자체를 분석하는 것과 비슷해진다.

###### 토픽 수가 너무 많으면 여러 토픽에 주요 단어가 중복되어 토픽마다의 개성이 드러나지 않게 된다.

###### 따라서 최적의 토픽 수를 찾는 것이 중요하다.<br/><br/>


#### DTM (Documetn Term Matrix, 문서 단어 행렬) 은 행은 문서, 열은 단어로 구성해 빈도를 나타낸 행렬이다. 

###### 즉, 문서 별 단어 빈도를 표현한다.

```{r}

# 문서별 단어 빈도 구하기
count_word_doc<- count_word %>%
  count(id, word, sort = T)

count_word_doc

# DTM (Documetn Term Matrix) 만들기
dtm_baemin <- count_word_doc %>%
  cast_dtm(document = id, term = word, value = n)

dtm_baemin

# dtm의 내용 확인하기 (15X15 행렬 형태로)
as.matrix(dtm_baemin[1:15, 1:15])

```


###### 20개의 LDA 모델을 일단 만들어준다.

###### 각 LDA 모델에 나오는 Griffiths2004 는 복잡도이다. 모델의 성능 지표가 되며, 텍스트를 얼마나 잘 설명하는지를 표현한다. 

###### 텍스트의 구조를 잘 설명할 수록 값이 커지게 된다.


```{r}
# 토픽 수 바꿔가면서 LDA 모델 여러개 만들기
models_baemin<- FindTopicsNumber(dtm = dtm_baemin,
                                topics = 2:20, # 20개의 LDA 모델을 만듦.
                                return_models = T,
                                control = list(seed = 1234))
models_baemin %>% 
  select(topics, Griffiths2004) #Griffiths2004 복잡도 

```


#### 성능 지표그래프를 그리면 최적 토픽 수에 대해 쉽게 파악할 수 있다.


###### 성능이 높을 수록 y값이 1에 가까워진다. X는 토픽의 수이다.

###### 토픽 수를 늘려도 더 이상 성능이 크게 향상되지 않고 등락 반복하기 시작하는 지점에서 토픽 수를 선택한다.

```{r}

# 성능 지표 그래프를 통해 최적 토픽 수 정하기
FindTopicsNumber_plot(models_baemin)

```


#### => 토픽의 수는 7개가 적합해 보인다. 

#### 또한, 토픽 수가 7개인 모델이 최적의 모델이 된다. 

```{r}

# 토픽 수가 7개인 모델 추출하기
optimal_model<-models_baemin %>%  
  filter(topics == 7) %>%
  pull(LDA_model) %>% # model 추출
  .[[1]] # list 추출

# 최적 모델 내용 확인
glimpse(optimal_model)

```



# 4. 찾은 각각의 토픽에 대해서 TF-IDF를구하고 각 토픽들의 중요단어 10개에 대한 막대그래프를 그리고 설명하시오. (10)


##### gamma란? 

##### : 문서가 각 토픽에 등장할 확률을 뜻한다.

###### gamma를 이용하면 원 문서를 토픽별로 구분할 수 있다.

###### 또한, 토픽의 주요 단어와 원문을 함께 살펴보면 토픽의 특징을 이해할 수 있다.



### 1) 먼저, 문서(리뷰) 별 topic을 지정하기 위해 gamma를 구해준다.

```{r}

# gamma (문서가 각 토픽에 등장할 확률) 추출 
doc_topic<-tidy(optimal_model, matrix = "gamma")
doc_topic

# gamma가 동점이 발생한 경우 확인하기
doc_topic %>%
  group_by(document) %>%
  top_n(n=1, wt=gamma) %>% 
  count(document) %>% 
  filter(n > 1) %>% print()
```

###### gamma가 동점인 경우 document, 즉 한 리뷰가 여러 토픽으로 분류될 수 있다는 의미이다. 

###### 무조건 하나의 리뷰가 하나의 토픽으로만 분류되게 처리해도 되지만, 굳이 처리하지 않아도 괜찮다. 

###### 해당 프로젝트에서는 따로 처리해주지 않는다.

```{r}

# 문서별 확률이 가장 높은 토픽 추출
doc_class <- doc_topic %>%
  group_by(document) %>%
  slice_max(gamma, n = 1) 
doc_class # 감마가 동점이 발생하는 경우가 있음. 

```


### 2) 다음으로, 원문 (리뷰 원문 기준) 에 확률 (gamma) 이 가장 높은 토픽의 번호 를 부여해준다.

###### 여기까지 진행하면 기존 원문에 topic과 gamma 값이 추가된다 

```{r}

# documnet 변수 타입을 정수형으로 통일 (데이터셋 결합하기 위해)
doc_class$document <- as.integer(doc_class$document) #데이터셋 변환 가능

# 원문에 감마 (확률) 값이 가장 높게 나온 토픽의 번호 부여
baemin_topic <- baemin %>%
  left_join(doc_class, by = c("id" = "document"))

# 결합 확인 (기존 원문에 topic, gamma 변수와 값이 추가됨)
baemin_topic %>%
  select(id, topic)
```

### 3) 그 다음, 토픽 별 문서 수를 살펴본다. 

###### topic이 NA인 문서, 즉 토픽으로 분류되지 않은 리뷰가 있는 것은 제거해준다.

```{r}

# 토픽별 문서 수 보기
baemin_topic %>%
  count(topic) #topic이 NA 인 문서가 13개 있음.

# topic이 NA인 문서 제거
baemin_topic <- baemin_topic %>%
  na.omit()

baemin_topic %>%
  count(topic)
```

#### => 이 과정이 종료되면 원문에 토픽 번호가 붙게 되고 특정 topic으로 분류되지 않은 문서가 제거된다. 리뷰마다 토픽에 맞게 분류된 것이다.


### 4) 원문이 토픽별로 분류되었으면 각각의 토픽별로 TF-IDF 를 구해준다.

##### TF-IDF란? 

##### : 어떤 단어가 흔하지 않으면서 (타 텍스트에서는 잘 나오지 않음), 특정 텍스트 (문서, 여기서는 리뷰이다.)에서 자주 사용되는지의 정도를 나타낸 지표이다.

###### 텍스트 (문서, document)의 개성을 드러내는 주요 단어 (특정 TOPIC에서 자주 사용되지만 다른 토픽에서는 잘 쓰이지 않는 단어) 를 찾는데 활용된다.

###### 즉, 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치이다.


```{r}

# 토픽번호가 붙은 원문 전처리 및 토큰화 진행
baemin_topic_token<-baemin_topic %>% 
  unnest_tokens(input = reply,
                output = word,
                token = extractNoun,
                drop = F) %>% 
  filter(str_count(word) > 1) # 두 글자 이상 추출
```

###### TF-IDF는 TOPIC의 개성을 드러내는 중요한 단어 (흔하지 않지만, 특정 TOPIC에서는 자주 사용되는 단어) 를 찾아주는 것이니 따로 주제어를 제거하지 않는다. 

###### 주제어일 경우 TF-IDF는 낮게 나올 것이다. <br/><br/>

###### TF-IDF는 TOPIC의 개성을 드러내는 중요한 단어 (흔하지 않지만, 특정 TOPIC에서는 자주 사용되는 단어) 를 찾아주는 것이니 stopword를 제거하지 않는다. 

###### stopword일 경우 TF-IDF는 낮게 나올 것이다.

```{r}

# 단어 빈도 구하기
freq <-baemin_topic_token %>%
  count(topic, word) %>%  print()

# tf-idf 구하기
freq <- freq %>% 
  bind_tf_idf(term = word, # 단어는 word 변수, 
              document = topic, # 텍스트 구분 기준은 topic,
              n = n) %>% # 단어 빈도
  arrange(-tf_idf)
freq

# 토픽별 주요 단어 10개 추출
top10_tf_idf<- freq%>%
  group_by(topic) %>%
  slice_max(tf_idf, n = 10, with_ties = F)


# 그래프 순서 지정
top10_tf_idf$topic <- factor(top10_tf_idf$topic,
                         levels = c(1:7)) # 토픽 1->7 순으로 지정


# 막대 그래프 그리기
top10_tf_idf %>% 
  ggplot(aes(x = reorder_within(x=word, by=tf_idf, within=topic),
             y = tf_idf,
             fill = topic)) +
  labs(x = "", y = "y는 tf-idf", title = "TOPIC별 중요한 단어", subtitle="(다른 TOPIC 대비 특정 TOPIC에서 강조한 단어)") +
  geom_text(aes(label = round(tf_idf,4) , hjust = -0.1),
            color = "dark grey", size=6)+ 
  geom_col(show.legend = F) +
  coord_flip() +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  scale_x_reordered() +
  labs(x = NULL) +
  theme(text = element_text(size=25))

```


#### => TF-IDF가 높은 단어들은 다른 토픽 대비 각 토픽에서 중요하게 쓰이는 단어이다. 

#### TF-IDF 를 이용할 경우 단순히 빈도수 (n) 가 큰 단어들을 나열하여 토픽의 특성을 파악하는 것보다 더 토픽의 고유한 특성에 대해 잘 파악할 수 있다. <br/><br/>

#### TF-IDF 로 구한 토픽별 중요 단어는 가중 로그 오즈비로 구한 토픽별 중요 단어와는 차이가 있다. 자세한 사항은 8번에서 설명한다. <br/><br/>


# 5. 각각의 토픽들에 대해서 중요단어를 나타내는 beta의 값이 큰 순서대로 워드클라우드를 그리고 설명을 하시오. (10)


##### beta란? 

##### :단어가 각 토픽에 들어갈 확률이다. 

###### beta를 통해 각 토픽에 등장할 가능성이 높은 주요 단어를 알 수 있다.

```{r}
# beta (단어가 각 토픽에 들어갈 확률) 추출하기
term_topic <- tidy(optimal_model, matrix = "beta") 
term_topic

```

###### beta를 구했다면 해당 값을 가지고 토픽별 워드 클라우드를 그려준다.

###### 워드 클라우드에 대한 자세한 설명은 2번에서 진행했으므로 생략한다. <br/><br/>

###### 2번 문제와 다른 점이 있다면 워드 클라우드를 구성하는는 단어의 크기를 빈도수(n) 이 아닌 beta*10000로 지정했다는 것이다.

###### 워드 클라우드에는 정확한 수치로 표현되지 않고, 빈도수 차이에 따라 단어 크기에 차이를 준 그림으로 표현된다. 

###### 때문에 워드 클라우드의 단어 크기를 지정할 때 '상대적인' 빈도수 (확률의 형태 등) 로 표현해주는 경우도 가능하다. 

###### 현재 항목에서 beta*10000 로 크기를 지정한것이 2번에서 size를 n으로 지정해준 것과 의미상 유사하게 , 시각적으로 큰 차이가 없게 작용하는 것이다.<br/><br/>


###### 참고로 각 word_cloud의 크기 (범위) 가 html로 확인할 시에는 다소 작게 표현되도록 지정하였다.

###### 여러 wordcloud를 한번에 비교하는 경우를 포함하였기에 range = c(5,25)로 지정한 것이다.


```{r}

# 워드 클라우드 그리기

#토픽 1의 워드 클라우드
term_topic1 <- term_topic %>% filter(topic == 1)
word1<-term_topic1 %>%
  ggplot(aes(label = term, size = beta*10000, # 크기는 beta*10000
             color = factor(sample.int(n=10,
                                       size=nrow(term_topic1), replace = TRUE)))) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(10, NA), # 최소 단어가 10번 이상 나와야 함.
               range = c(5,25)) +
  ggtitle("TOPIC 1")+
  theme_minimal()

word1

#토픽 2의 워드 클라우드
term_topic2 <- term_topic %>% filter(topic == 2)
word2<-term_topic2 %>%
  ggplot(aes(label = term, size = beta*10000, # 크기는 beta*10000
             color = factor(sample.int(n=10,
                                       size=nrow(term_topic2), replace = TRUE)))) + 
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(10, NA),  # 최소 단어가 10번 이상 나와야 함.
               range = c(5,25)) +
  ggtitle("TOPIC 2")+
  theme_minimal() 

word2

#토픽 3의 워드 클라우드
term_topic3<- term_topic %>% filter(topic ==3)
word3<-term_topic3 %>%
  ggplot(aes(label = term, size = beta*10000, # 크기는 beta*10000
             color = factor(sample.int(n=10,
                                       size=nrow(term_topic3), replace = TRUE)))) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(5,25),  # 최소 단어가 10번 이상 나와야 함.
               range = c(3, 15)) +
  ggtitle("TOPIC 3")+
  theme_minimal()

word3

#토픽 4의 워드 클라우드
term_topic4 <- term_topic %>% filter(topic == 4)
word4<-term_topic4 %>%
  ggplot(aes(label = term, size = beta*10000, # 크기는 beta*10000
             color = factor(sample.int(n=10,
                                       size=nrow(term_topic4), replace = TRUE)))) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(10, NA),  # 최소 단어가 10번 이상 나와야 함.
               range = c(5,25)) +
  ggtitle("TOPIC 4")+
  theme_minimal()

word4

#토픽 5의 워드 클라우드
term_topic5 <- term_topic %>% filter(topic == 5)
word5<-term_topic5 %>%
  ggplot(aes(label = term, size = beta*10000, # 크기는 beta*10000
             color = factor(sample.int(n=10,
                                       size=nrow(term_topic5), replace = TRUE)))) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(10, NA),  # 최소 단어가 10번 이상 나와야 함.
               range = c(5,25)) +
  ggtitle("TOPIC 5")+
  theme_minimal()

word5

#토픽 6의 워드 클라우드
term_topic6 <- term_topic %>% filter(topic == 6)
word6<-term_topic6 %>%
  ggplot(aes(label = term, size = beta*10000, # 크기는 beta*10000
             color = factor(sample.int(n=10,
                                       size=nrow(term_topic6), replace = TRUE)))) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(10, NA),  # 최소 단어가 10번 이상 나와야 함.
               range = c(5,25)) +
  ggtitle("TOPIC 6")+
  theme_minimal()

word6

#토픽 7의 워드 클라우드
term_topic7 <- term_topic %>% filter(topic == 7)
word7<-term_topic7 %>%
  ggplot(aes(label = term, size = beta*10000, # 크기는 beta*10000
             color = factor(sample.int(n=10,
                                       size=nrow(term_topic7), replace = TRUE)))) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(10, NA), # 최소 단어가 10번 이상 나와야 함.
               range = c(5,25)) +
  ggtitle("TOPIC 7")+
  theme_minimal()

word7

# 워드클라우드 여러개를 동시에 나타내기

grid.arrange(word1,word2,word3,word4,ncol=2)
grid.arrange(word5,word6,word7, ncol=2)

```

#### => 워드 클라우드 확인 결과 토픽 1의 주요 단어는 '결제', '번호', '문제' , '로그인', '인증' 등으로 나타난다.

#### => 워드 클라우드 확인 결과 토픽 2의 주요 단어는 '리뷰', '주문', '음식', 음식', '가게' 등으로 나타난다.

#### => 워드 클라우드 확인 결과 토픽 3의 주요 단어는 '단어', '검색기능', '메뉴판', '공지', '잘못' 등으로 나타난다. 
###### 7개의 토픽 중 topic 3이 가장 워드 클라우드만을 보고 중요한 단어가 무엇인지 판단하기가 어렵다. 
###### 여러개의 단어들의 beta값이 비슷하게 나와 가장 핵심적인 단어가 무엇인지 한눈에 확인하기 비교적 어렵다.

#### => 워드 클라우드 확인 결과 토픽 4의 주요 단어는 '이벤트', '쿠폰', '이용', '요기요', '할인' 등으로 나타난다.

### => 워드 클라우드 확인 결과 토픽 5의 주요 단어는 '사용', '어플', '불편' 등으로 나타난다.

#### => 워드 클라우드 확인 결과 토픽 6의 주요 단어는 '수수료', '삭제', '탈퇴', '독과점' 등으로 나타난다. 
###### 타 토픽 대비 부정적인 감정과 행동이 나타내는 단어가 많은 것으로 확인된다. 

#### => 워드 클라우드 확인 결과 토픽 7의 주요 단어는 '주문', '업데이트', '확인', '금액', '카드' 등으로 나타난다. <br/><br/>


#### => 7개 토픽 각각의 워드 클라우드를 확인한 결과, 전체 리뷰의 워드 클라우드 (2번 참고) 에서 중요 단어로 나타났던 '배달', '주문','어플', '사용', '리뷰', '업데이트' 등의 단어들이 각 토픽별 워드 클라우드에 골고루 배분되어 있다. 

#### 또한 해당 단어들 각각은 분류된 토픽은 다를지라도 각 토픽에서 중요 단어인 것이 확인된다. <br/><br/>



# 6. 각각의 토픽들에 대해서 중요단어를 나타내는 beta의 값의 크기에 따른 그래프를 그리고 설명하시오. (10)   


###### 5번 항목에서 beta를 추출한 term_topic을 사용한다. 

###### beta에 대한 설명도 5번 항목에서 자세하게 진행했으므로 생략한다.

```{r}

# 토픽별 beta가 높은 상위 10개 단어 추출
top10_term_topic<- term_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = F) %>% # 동점 제외
  ungroup() %>%
  arrange(topic, -beta)

top10_term_topic

# 막대 그래프로 나타내기
top10_term_topic %>% 
  ggplot(aes(x = reorder_within(term, beta, topic),
             y = beta,
             fill = factor(topic))) +
  labs(x = "", y = "y는 beta", title = "TOPIC별 중요한 단어", subtitle="(토픽별 각 토픽에 등장할 확률이 높은 단어)") +
  geom_text(aes(label = round(beta,4) , hjust = -0.2),
            color = "dark grey", size=6)+ 
  geom_col(show.legend = F) +
  facet_wrap(~ topic, scales = "free",ncol = 4) +
  coord_flip() +
  scale_x_reordered() +
  theme(text = element_text(size=25))

```

#### => 각 토픽에 등장할 확률이 높은 단어 (beta 값 이용) 에 대해 토픽별로 분류해서 한 번에 나타내었다.

#### =>beta를 이용해서 각 토픽별 중요 단어를 분석한 결과는 TF-IDF를 사용해서 각 토픽별 주요단어를 표현한 것과는 차이가 있다는 것이 확인된다. (4번 항목 참고) <br/><br/>





# 7. 찾은 각각의 토픽의 집단 내에서의 감정분석을 하고 설명을 하시오. (10)

##### 감정 분석이란? 

##### : 텍스트에 어떤 감정이 담겨있는지 분석하는 방법이다.

##### 사람들이 어떤 주제를 긍정적 / 부정적으로 느끼는지를 파악할 수 있다.<br/><br/>

###### 감정 분석을 위해서는 인간의 '감정'에 정의한 데이터가 필요하다. 

###### 즉, 해당 단어가 감정에 대한 단어인지, 감정을 나타낸 단어가 맞다면 긍정 단어인지, 부정 단어인지를 파악할 수 있어야 한다. <br/><br/>

###### 이러한 감정의 파악을 위해 감정 사전을 사용한다. 감정 사전은 감정 단어'와 '감정의 강도를 표현한 숫자'로 구성된 사전이다.

###### 어떤 것을 분석하는지에 따라 차이가 있으나 기본적으로 감정 사전을 이용해 문장의 단어에 감정 점수를 부여한 다음 합산하는 방식을 주로 이용한다.<br/><br/>


###### 해당 프로젝트에서는 'KNU 한국어 감성사전'을 사용한다.

```{r}

# 감정 사전 불러오기 (KNU 한국어 감성사전 사용)
senti_dic <- read_delim('C:/knu_senti_dict-master/knu_senti_dict-master/SentiWord_Dict.txt', delim='\t', col_names=c("word", "polarity"))

# 감정 사전 탐색
head(senti_dic,10)
dim(senti_dic)
table(senti_dic$polarity)

# 완전 긍정 (2점) 단어 보기
senti_dic %>%
  filter(polarity == 2) %>%
  arrange(word)

# 완전 부정 (-2점) 단어 보기
senti_dic %>%
  filter(polarity == -2) %>%
  arrange(word)

# 이모티콘 보기
senti_dic %>%
  filter(!str_detect(word, "[가-힣]")) %>%
  arrange(word)

# 감정사전 내 긍정 단어, 중립, 부정 단어의 수 보기
senti_dic %>%
  mutate(sentiment = ifelse(polarity >= 1, "pos",
                            ifelse(polarity <= -1, "neg", 
                                   "neu"))) %>%
  count(sentiment)

# 감정사전 내 완전 긍정 단어, 중립 (완전 긍정과 완전 부정이 아니면 모두 중립), 완전 부정 단어의 수 보기
senti_dic %>%
  mutate(sentiment = ifelse(polarity == 2, "pos",
                            ifelse(polarity == -2, "neg", 
                                   "neu"))) %>%
  count(sentiment)

```


##### 지금부터 토픽별 감정 분석을 진행한다.

##### 감정 분석은 크게 1. 토픽별 자주 사용하는 긍정/부정 단어 확인 , 2. 토픽 별 감정 경향 확인 , 3. 토픽별 긍정/부정 리뷰에서 자주 사용하는 단어 확인 등의 형태로 진행된다.<br/><br/>

##### 따라서 토픽 1부터 토픽 7까지 앞서 언급한 3가지 헝태의 감정 분석을 진행할 것이다. 

###### 토픽 1의 감정 분석을 모두 마친 후 다음 토픽의 감정 분석을 진행할 예정이며, 한 토픽의 감정 분석이 끝난 후 다음 토픽 감정 분석을 전개하는 비슷한 과정이 반복된다.

###### 따라서 토픽 1만 상세하게 설명을 진행할 예정이며, 토픽 2부터 7까지는 간단히 주석만 달 예정이다.


### 0) 전처리 진행 (전 토픽 공통)

###### 앞선 과정에서 이미 extractNoun 기준으로 토큰화를 진행하였다.

###### 그러나 이는 형태소 분석 처리된 명사 형태 (baemin_topic_token의  word 변수)이다. 

###### 따라서, extractNoun 기준으로 토큰화가 진행할 경우 동사가 누락될 수 있다.<br/><br/> 

###### 따라서 words (단어, 띄어쓰기로 분리됨) 기준으로 토큰화를 진행한다.

```{r}

# topic이 포함된 원문 token화 진행.
word_baemin<-baemin_topic_token %>%
  unnest_tokens(input = reply, 
                output = word, 
                token = "words", drop = F) %>%
  filter(str_count(word) > 1) %>% print() # 두 글자 이상 추출

```

###### extractNoun를 기준으로 토큰화를 진행한 후의 감정 분석 결과와 words를 기준으로 토큰화를 진행한 후의 감정 분석 결과는 큰 차이가 있었다. 

###### extractNoun로 진행할 경우 제거되는 단어들이 너무 많았다.

###### 반면, words로 진행할 경우에는 행태소 분석이 되지 않아 유의어가 너무 많았다. <br/><br/>

###### 따라서 본 연구자는 해당 프로젝트에서 words를 기준으로 하여 token화를 진행하는 방식을 선택하였다.

###### 유의어가 발생하는 경우 해석에 주의를 하면 분석 결과에 대해 파악할 수 있으나 단어가 제거되는 경우 보다 왜곡된 결과를 가져올 수 있다고 판단하였기 때문이다.

###### 따라서, 본 수업 및 교재에 나온 것과 같이 words를 기준으로 하여 token화를 진행하였으며 유의어 및 stopword에 주의하여 분석 결과를 해석하였다.


## 1-1) 토픽1의 최빈 단어별 감정분석 

###### 먼저, 토큰화된 데이터에서 토픽 1만을 선택해준다. 

```{r}
# 토픽1 선택
topic1<-word_baemin %>% 
  filter(topic==1) %>% 
  select(topic, id, word, reply ) %>% print()
```

###### 감정사전 senti_dic을 활용하여 각 단어별 감정 점수를 부여한다.


###### 감정 사전에 없는 단어는 0점이다. (중립)

```{r}
# 감정점수 부여
topic1 <- topic1 %>%
  left_join(senti_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) # 사전에 없는 단어의 점수는 0

topic1 %>% 
  dplyr::select(word, polarity)
```

###### 감정이 분명한 단어를 살펴보기 위해 완전긍정 (2점. 그냥 긍정이라 칭하겠다.), 완전부정 (-2점, 그냥 부정이라 칭하겠다.), 중립 (-1,0,1점, 긍정 및 부정으로 분류되지 않은 것들이 속하게 된다.) 으로 단어를 분류한다.

```{r}
# 감정분류
topic1 <- topic1 %>%
  mutate(sentiment = ifelse(polarity ==2, "pos",
                            ifelse(polarity == -2, "neg", "neu")))

topic1 %>%
  count(sentiment)
```

###### 해당 토픽에서 중립을 제외하고 완전긍정, 완전부정 단어가 얼마나 사용되었는지를 확인한다.

```{r}
# 긍정/부정 상위 10개 단어보기
top10_sentiment1 <- topic1 %>%
  filter(sentiment != "neu") %>% # 중립 단어 제외
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% print(n=Inf) # 중복 허용

# 막대 그래프 그리기
top10_sentiment1 %>% 
  ggplot(aes(x = reorder(word, n),
             y = n,
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1,
            color = "dark grey", size=6) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.25))) +
  labs(x = NULL,y = "빈도수 (n)", title = "TOPIC 1에서 자주 사용된 감정단어 ") +
  theme(text = element_text(size=25))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))

```


## 1-2) 토픽1의 전체적인 감정 경향 구하기


###### 먼저, 해당 토픽내 리뷰별 감정 점수를 구한다.

###### 리뷰별 감정 점수는 id (리뷰별 구분), reply별로 분리한 다음 polarity를 합산하였다. 

```{r}
# 리뷰별 감정 점수 
score_comment1 <- topic1 %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

# 긍정 점수가 높은 리뷰보기 
score_comment1 %>%
  select(score, reply) %>% 
  arrange(-score)

# 부정 점수가 높은 리뷰보기
score_comment1 %>%
  select(score, reply) %>% 
  arrange(score)

# 감정 점수 빈도 
# 전체적으로 0점이 부여된 리뷰이 가장 많고, 긍정과 부정의 양 극단으로 갈 수록 빈도가 감소함
score_comment1 %>%
  count(score)

# 감정 분류하기
score_comment1 <- score_comment1 %>%
  mutate(sentiment = ifelse(score >= 1, "pos",
                            ifelse(score <= -1, "neg", "neu")))

# 감정 빈도 및 비율 구하기
freq_score1 <- score_comment1 %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100) %>% print()

# 막대 그래프 만들기
ggplot(freq_score1, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3, size=20) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))+ # 축 순서 정하기
  theme(legend.box.background = element_rect(fill = "pink"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "감정 단어 유형", y = "빈도수 (n)", title = "TOPIC 1에서 사용된 감정 단어 유형")+
  theme(text = element_text(size=40))

```

###### 이번에는 누적 막대그래프 형태의 비율로 감정비율을 표현한다.

###### 이를 위해서는 x축과 y축, 누적막대를 표현할 변수가 필요하다. 따라서, x축을 구성할 더미 변수 (가변수) 를 추가한다

```{r}
# 비율 누적 막대그래프 만들기 - 감정비율 표현
# 더미 변수 생성
freq_score1$dummy <- 0

ggplot(freq_score1, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")), # % 표시
            position = position_stack(vjust = 0.5), # 가운데에 텍스트 표시
            size=20) +
  theme(axis.title.x = element_blank(), # x축 이름 삭제
        axis.text.x = element_blank(), # x축 값 삭제
        axis.ticks.x = element_blank())+ # x축 값 삭제
  theme(legend.box.background = element_rect(fill = "skyblue"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(y = "비율 (총 100%)", title = "TOPIC 1에서 사용된 감정 단어 유형-비율")+
  theme(text = element_text(size=40))

```



## 1-3) 토픽1의 감정 범주 (긍정 /부정) 별 많이 사용되는 단어

###### 리뷰별 감정점수 변수가 추가된 score_comment1 사용한다. 토큰화를 진행한다.

```{r}
# score_comment를 토큰화 진행
comment1 <- score_comment1 %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words", # extractNoun이라고 지정할 경우 명사만 나옴. 동사도 나와야하므로 그냥 word를 해줌
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") & # 한글 추출
           str_count(word) >= 2) # 두 글자 이상 추출
head(comment1)

```

###### 다음으로 각 감정 (긍정, 부정, 중립) 단어의 어떤 단어가 많이 쓰였는지 (빈도) 를 구해준다.

```{r}

# 감정 및 단어별 빈도 구하기 
freq_word1 <- comment1 %>%
  filter(str_count(word) >= 2) %>% # 2번 이상 등장하는 단어만
  count(sentiment, word, sort = T) %>% print()


# 긍정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word1 %>%
  filter(sentiment == "pos")

# 부정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word1 %>%
  filter(sentiment == "neg")
```

###### 감정 댓글별 자주 사용된 단어와 빈도수를 구했다면 로그 오즈비를 활용해서 상대적으로 더 자주 사용된 단어가 무엇인지 비교해준다.<br/><br/>

##### 로즈오즈비란? 

##### : 로그 상대효율이라 한다. 오즈비에 로그를 취한 것이다,

###### 단어의 오즈비가 1보다 크면 + , 1보다 작으면 -가 되며, 단어가 두 텍스트 중 어디에서 비중이 큰지에 따라 서로 다른 부호를 갖는다. 텍스트 차이 분명하게 드러나도록 시각화하는데 활용한다.

###### 즉, 부호와 크기를 보면 단어가 어느 연설문에서 더 중요한지 알 수 있는 것이다. 

```{r}
# 로그오즈비 계산하기
comment_wide1 <- freq_word1 %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment, # sentiment의 범주를 변수로 사용
              values_from = n,
              values_fill = list(n = 0)) %>% print() # 단어 없으면 0

# 로그 오즈비 구하는 공식
comment_wide1 <- comment_wide1 %>%
  mutate(log_RR = log(((pos + 1) / (sum(pos + 1))) / # pos에서의 단어의 비중
                        ((neg + 1) / (sum(neg + 1))))) # neg에서의 단어의 비중 => 에 로그 취한값
comment_wide1

# 로그오즈비가 가장 큰 단어 10개 추출 
top10_topic1 <- comment_wide1 %>%
  group_by(sentiment = ifelse(log_RR > 0, "pos", "neg")) %>%
  slice_max(abs(log_RR), n = 10, with_ties = F) # 로그RR 동점 단어 제외
top10_topic1

#막대 그래프 그리기

ggplot(top10_topic1, aes(x = reorder(word, log_RR), y = log_RR, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(size=10))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "", y = "y는 로그-오즈비 (logRR)", title = "긍정/부정 리뷰에서 자주 사용된 단어", subtitle="TOPIC 1")+
  theme(text = element_text(size=25))
  

```

###### 여기까지가 토픽 1의 감정분석 과정이다. 

##### 토픽 2부터 토픽 7까지도 동일한 방식으로 감정 분석을 진행한다.



## 2-1) 토픽2의 최빈 단어별 감정분석 


```{r}
# 토픽2 선택
topic2<-word_baemin %>% 
  filter(topic==2) %>% 
  select(topic, id, word, reply ) %>% print()

# 감정점수 부여
topic2 <- topic2 %>%
  left_join(senti_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) # 사전에 없는 단어의 점수는 0

topic2 %>% 
  dplyr::select(word, polarity)

# 감정분류
topic2 <- topic2 %>%
  mutate(sentiment = ifelse(polarity ==2, "pos",
                            ifelse(polarity == -2, "neg", "neu")))

topic2 %>%
  count(sentiment)

# 긍정/부정 상위 10개 단어보기
top10_sentiment2 <- topic2 %>%
  filter(sentiment != "neu") %>% # 중립 단어 제외
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% print(n=Inf) # 중복 허용


# 막대 그래프 그리기
top10_sentiment2 %>% 
  ggplot(aes(x = reorder(word, n),
             y = n,
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1,
            color = "dark grey", size=6) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.25))) +
  labs(x = NULL,y = "빈도수 (n)", title = "TOPIC 2에서 자주 사용된 감정단어 ") +
  theme(text = element_text(size=25))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))

```

## 2-2) 토픽2의 전체적인 감정 경향 구하기



```{r}
# 리뷰별 감정 점수 
score_comment2 <- topic2 %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

# 긍정 점수가 높은 리뷰보기 
score_comment2 %>%
  select(score, reply) %>% 
  arrange(-score)

# 부정 점수가 높은 리뷰보기
score_comment2 %>%
  select(score, reply) %>% 
  arrange(score)

# 감정 점수 빈도 
# 전체적으로 0점이 부여된 리뷰이 가장 많고, 긍정과 부정의 양 극단으로 갈 수록 빈도가 감소함
score_comment2 %>%
  count(score)

# 감정 분류하기
score_comment2 <- score_comment2 %>%
  mutate(sentiment = ifelse(score >= 1, "pos",
                            ifelse(score <= -1, "neg", "neu")))

# 감정 빈도 및 비율 구하기
freq_score2 <- score_comment2 %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100) %>% print()

# 막대 그래프 만들기
ggplot(freq_score2, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3, size=20) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))+ # 축 순서 정하기
  theme(legend.box.background = element_rect(fill = "pink"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "감정 단어 유형", y = "빈도수 (n)", title = "TOPIC 2에서 사용된 감정 단어 유형")+
  theme(text = element_text(size=40))


# 비율 누적 막대그래프 만들기 - 감정비율 표현
# 더미 변수 생성
freq_score2$dummy <- 0

ggplot(freq_score2, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")), # % 표시
            position = position_stack(vjust = 0.5), # 가운데에 텍스트 표시
            size=20) +
  theme(axis.title.x = element_blank(), # x축 이름 삭제
        axis.text.x = element_blank(), # x축 값 삭제
        axis.ticks.x = element_blank())+ # x축 값 삭제
  theme(legend.box.background = element_rect(fill = "skyblue"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(y = "비율 (총 100%)", title = "TOPIC 2에서 사용된 감정 단어 유형-비율")+
  theme(text = element_text(size=40))

```



## 2-3) 토픽2의 감정 범주 (긍정 /부정) 별 많이 사용되는 단어


```{r}
# score_comment를 토큰화 진행
comment2 <- score_comment2 %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words", # extractNoun이라고 지정할 경우 명사만 나옴. 동사도 나와야하므로 그냥 word를 해줌
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") & # 한글 추출
           str_count(word) >= 2) # 두 글자 이상 추출
head(comment2)


# 감정 및 단어별 빈도 구하기 
freq_word2 <- comment2 %>%
  filter(str_count(word) >= 2) %>% # 2번 이상 등장하는 단어만
  count(sentiment, word, sort = T) %>% print()


# 긍정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word2 %>%
  filter(sentiment == "pos")

# 부정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word2 %>%
  filter(sentiment == "neg")

# 로그오즈비 계산하기
comment_wide1 <- freq_word2 %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment, # sentiment의 범주를 변수로 사용
              values_from = n,
              values_fill = list(n = 0)) %>% print() # 단어 없으면 0

# 로그 오즈비 구하는 공식
comment_wide1 <- comment_wide1 %>%
  mutate(log_RR = log(((pos + 1) / (sum(pos + 1))) / # pos에서의 단어의 비중
                        ((neg + 1) / (sum(neg + 1))))) # neg에서의 단어의 비중 => 에 로그 취한값
comment_wide1

# 로그오즈비가 가장 큰 단어 10개 추출 
top10_topic2 <- comment_wide1 %>%
  group_by(sentiment = ifelse(log_RR > 0, "pos", "neg")) %>%
  slice_max(abs(log_RR), n = 10, with_ties = F) # 로그RR 동점 단어 제외
top10_topic2

#막대 그래프 그리기

ggplot(top10_topic2, aes(x = reorder(word, log_RR), y = log_RR, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(size=10))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "", y = "y는 로그-오즈비 (logRR)", title = "긍정/부정 리뷰에서 자주 사용된 단어", subtitle="TOPIC 2")+
  theme(text = element_text(size=25))
```


## 3-1) 토픽3의 최빈 단어별 감정분석 


```{r}
# 토픽3 선택
topic3<-word_baemin %>% 
  filter(topic==3) %>% 
  select(topic, id, word, reply ) %>% print()

# 감정점수 부여
topic3 <- topic3 %>%
  left_join(senti_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) # 사전에 없는 단어의 점수는 0

topic3 %>% 
  dplyr::select(word, polarity)

# 감정분류
topic3 <- topic3 %>%
  mutate(sentiment = ifelse(polarity ==2, "pos",
                            ifelse(polarity == -2, "neg", "neu")))

topic3 %>%
  count(sentiment)

# 긍정/부정 상위 10개 단어보기
top10_sentiment3 <- topic3 %>%
  filter(sentiment != "neu") %>% # 중립 단어 제외
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% print(n=Inf) # 중복 허용


# 막대 그래프 그리기
top10_sentiment3 %>% 
  ggplot(aes(x = reorder(word, n),
             y = n,
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1,
            color = "dark grey", size=6) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.25))) +
  labs(x = NULL,y = "빈도수 (n)", title = "TOPIC 3에서 자주 사용된 감정단어 ") +
  theme(text = element_text(size=25))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))

```

## 3-2) 토픽3의 전체적인 감정 경향 구하기



```{r}
# 리뷰별 감정 점수 
score_comment3 <- topic3 %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

# 긍정 점수가 높은 리뷰보기 
score_comment3 %>%
  select(score, reply) %>% 
  arrange(-score)

# 부정 점수가 높은 리뷰보기
score_comment3 %>%
  select(score, reply) %>% 
  arrange(score)

# 감정 점수 빈도 
# 전체적으로 0점이 부여된 리뷰이 가장 많고, 긍정과 부정의 양 극단으로 갈 수록 빈도가 감소함
score_comment3 %>%
  count(score)

# 감정 분류하기
score_comment3 <- score_comment3 %>%
  mutate(sentiment = ifelse(score >= 1, "pos",
                            ifelse(score <= -1, "neg", "neu")))

# 감정 빈도 및 비율 구하기
freq_score3 <- score_comment3 %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100) %>% print()

# 막대 그래프 만들기
ggplot(freq_score3, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3, size=20) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))+ # 축 순서 정하기
  theme(legend.box.background = element_rect(fill = "pink"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "감정 단어 유형", y = "빈도수 (n)", title = "TOPIC 3에서 사용된 감정 단어 유형")+
  theme(text = element_text(size=40))


# 비율 누적 막대그래프 만들기 - 감정비율 표현
# 더미 변수 생성
freq_score3$dummy <- 0

ggplot(freq_score3, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")), # % 표시
            position = position_stack(vjust = 0.5), # 가운데에 텍스트 표시
            size=20) +
  theme(axis.title.x = element_blank(), # x축 이름 삭제
        axis.text.x = element_blank(), # x축 값 삭제
        axis.ticks.x = element_blank())+ # x축 값 삭제
  theme(legend.box.background = element_rect(fill = "skyblue"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(y = "비율 (총 100%)", title = "TOPIC 3에서 사용된 감정 단어 유형-비율")+
  theme(text = element_text(size=40))

```



## 3-3) 토픽3의 감정 범주 (긍정 /부정) 별 많이 사용되는 단어


```{r}
# score_comment를 토큰화 진행
comment3 <- score_comment3 %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words", # extractNoun이라고 지정할 경우 명사만 나옴. 동사도 나와야하므로 그냥 word를 해줌
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") & # 한글 추출
           str_count(word) >= 2) # 두 글자 이상 추출
head(comment3)


# 감정 및 단어별 빈도 구하기 
freq_word3 <- comment3 %>%
  filter(str_count(word) >= 2) %>% # 2번 이상 등장하는 단어만
  count(sentiment, word, sort = T) %>% print()


# 긍정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word3 %>%
  filter(sentiment == "pos")

# 부정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word3 %>%
  filter(sentiment == "neg")

# 로그오즈비 계산하기
comment_wide1 <- freq_word3 %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment, # sentiment의 범주를 변수로 사용
              values_from = n,
              values_fill = list(n = 0)) %>% print() # 단어 없으면 0

# 로그 오즈비 구하는 공식
comment_wide1 <- comment_wide1 %>%
  mutate(log_RR = log(((pos + 1) / (sum(pos + 1))) / # pos에서의 단어의 비중
                        ((neg + 1) / (sum(neg + 1))))) # neg에서의 단어의 비중 => 에 로그 취한값
comment_wide1

# 로그오즈비가 가장 큰 단어 10개 추출 
top10_topic3 <- comment_wide1 %>%
  group_by(sentiment = ifelse(log_RR > 0, "pos", "neg")) %>%
  slice_max(abs(log_RR), n = 10, with_ties = F) # 로그RR 동점 단어 제외
top10_topic3

#막대 그래프 그리기

ggplot(top10_topic3, aes(x = reorder(word, log_RR), y = log_RR, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(size=10))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "", y = "y는 로그-오즈비 (logRR)", title = "긍정/부정 리뷰에서 자주 사용된 단어", subtitle="TOPIC 3")+
  theme(text = element_text(size=25))
```


## 4-1) 토픽4의 최빈 단어별 감정분석 


```{r}
# 토픽4 선택
topic4<-word_baemin %>% 
  filter(topic==4) %>% 
  select(topic, id, word, reply ) %>% print()

# 감정점수 부여
topic4 <- topic4 %>%
  left_join(senti_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) # 사전에 없는 단어의 점수는 0

topic4 %>% 
  dplyr::select(word, polarity)

# 감정분류
topic4 <- topic4 %>%
  mutate(sentiment = ifelse(polarity ==2, "pos",
                            ifelse(polarity == -2, "neg", "neu")))

topic4 %>%
  count(sentiment)

# 긍정/부정 상위 10개 단어보기
top10_sentiment4 <- topic4 %>%
  filter(sentiment != "neu") %>% # 중립 단어 제외
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% print(n=Inf) # 중복 허용


# 막대 그래프 그리기
top10_sentiment4 %>% 
  ggplot(aes(x = reorder(word, n),
             y = n,
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1,
            color = "dark grey", size=6) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.25))) +
  labs(x = NULL,y = "빈도수 (n)", title = "TOPIC 4에서 자주 사용된 감정단어 ") +
  theme(text = element_text(size=25))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))

```

## 4-2) 토픽4의 전체적인 감정 경향 구하기



```{r}
# 리뷰별 감정 점수 
score_comment4 <- topic4 %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

# 긍정 점수가 높은 리뷰보기 
score_comment4 %>%
  select(score, reply) %>% 
  arrange(-score)

# 부정 점수가 높은 리뷰보기
score_comment4 %>%
  select(score, reply) %>% 
  arrange(score)

# 감정 점수 빈도 
# 전체적으로 0점이 부여된 리뷰이 가장 많고, 긍정과 부정의 양 극단으로 갈 수록 빈도가 감소함
score_comment4 %>%
  count(score)

# 감정 분류하기
score_comment4 <- score_comment4 %>%
  mutate(sentiment = ifelse(score >= 1, "pos",
                            ifelse(score <= -1, "neg", "neu")))

# 감정 빈도 및 비율 구하기
freq_score4 <- score_comment4 %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100) %>% print()

# 막대 그래프 만들기
ggplot(freq_score4, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3, size=20) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))+ # 축 순서 정하기
  theme(legend.box.background = element_rect(fill = "pink"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "감정 단어 유형", y = "빈도수 (n)", title = "TOPIC 4에서 사용된 감정 단어 유형")+
  theme(text = element_text(size=40))


# 비율 누적 막대그래프 만들기 - 감정비율 표현
# 더미 변수 생성
freq_score4$dummy <- 0

ggplot(freq_score4, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")), # % 표시
            position = position_stack(vjust = 0.5), # 가운데에 텍스트 표시
            size=20) +
  theme(axis.title.x = element_blank(), # x축 이름 삭제
        axis.text.x = element_blank(), # x축 값 삭제
        axis.ticks.x = element_blank())+ # x축 값 삭제
  theme(legend.box.background = element_rect(fill = "skyblue"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(y = "비율 (총 100%)", title = "TOPIC 4에서 사용된 감정 단어 유형-비율")+
  theme(text = element_text(size=40))

```



## 4-3) 토픽4의 감정 범주 (긍정 /부정) 별 많이 사용되는 단어


```{r}
# score_comment를 토큰화 진행
comment4 <- score_comment4 %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words", # extractNoun이라고 지정할 경우 명사만 나옴. 동사도 나와야하므로 그냥 word를 해줌
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") & # 한글 추출
           str_count(word) >= 2) # 두 글자 이상 추출
head(comment4)


# 감정 및 단어별 빈도 구하기 
freq_word4 <- comment4 %>%
  filter(str_count(word) >= 2) %>% # 2번 이상 등장하는 단어만
  count(sentiment, word, sort = T) %>% print()


# 긍정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word4 %>%
  filter(sentiment == "pos")

# 부정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word4 %>%
  filter(sentiment == "neg")

# 로그오즈비 계산하기
comment_wide1 <- freq_word4 %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment, # sentiment의 범주를 변수로 사용
              values_from = n,
              values_fill = list(n = 0)) %>% print() # 단어 없으면 0

# 로그 오즈비 구하는 공식
comment_wide1 <- comment_wide1 %>%
  mutate(log_RR = log(((pos + 1) / (sum(pos + 1))) / # pos에서의 단어의 비중
                        ((neg + 1) / (sum(neg + 1))))) # neg에서의 단어의 비중 => 에 로그 취한값
comment_wide1

# 로그오즈비가 가장 큰 단어 10개 추출 
top10_topic4 <- comment_wide1 %>%
  group_by(sentiment = ifelse(log_RR > 0, "pos", "neg")) %>%
  slice_max(abs(log_RR), n = 10, with_ties = F) # 로그RR 동점 단어 제외
top10_topic4

#막대 그래프 그리기

ggplot(top10_topic4, aes(x = reorder(word, log_RR), y = log_RR, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(size=10))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "", y = "y는 로그-오즈비 (logRR)", title = "긍정/부정 리뷰에서 자주 사용된 단어", subtitle="TOPIC 4")+
  theme(text = element_text(size=25))
```


## 5-1) 토픽5의 최빈 단어별 감정분석 


```{r}
# 토픽5 선택
topic5<-word_baemin %>% 
  filter(topic==5) %>% 
  select(topic, id, word, reply ) %>% print()

# 감정점수 부여
topic5 <- topic5 %>%
  left_join(senti_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) # 사전에 없는 단어의 점수는 0

topic5 %>% 
  dplyr::select(word, polarity)

# 감정분류
topic5 <- topic5 %>%
  mutate(sentiment = ifelse(polarity ==2, "pos",
                            ifelse(polarity == -2, "neg", "neu")))

topic5 %>%
  count(sentiment)

# 긍정/부정 상위 10개 단어보기
top10_sentiment5 <- topic5 %>%
  filter(sentiment != "neu") %>% # 중립 단어 제외
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% print(n=Inf) # 중복 허용


# 막대 그래프 그리기
top10_sentiment5 %>% 
  ggplot(aes(x = reorder(word, n),
             y = n,
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1,
            color = "dark grey", size=6) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.25))) +
  labs(x = NULL,y = "빈도수 (n)", title = "TOPIC 5에서 자주 사용된 감정단어 ") +
  theme(text = element_text(size=25))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))

```

## 5-2) 토픽5의 전체적인 감정 경향 구하기



```{r}
# 리뷰별 감정 점수 
score_comment5 <- topic5 %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

# 긍정 점수가 높은 리뷰보기 
score_comment5 %>%
  select(score, reply) %>% 
  arrange(-score)

# 부정 점수가 높은 리뷰보기
score_comment5 %>%
  select(score, reply) %>% 
  arrange(score)

# 감정 점수 빈도 
# 전체적으로 0점이 부여된 리뷰이 가장 많고, 긍정과 부정의 양 극단으로 갈 수록 빈도가 감소함
score_comment5 %>%
  count(score)

# 감정 분류하기
score_comment5 <- score_comment5 %>%
  mutate(sentiment = ifelse(score >= 1, "pos",
                            ifelse(score <= -1, "neg", "neu")))

# 감정 빈도 및 비율 구하기
freq_score5 <- score_comment5 %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100) %>% print()

# 막대 그래프 만들기
ggplot(freq_score5, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3, size=20) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))+ # 축 순서 정하기
  theme(legend.box.background = element_rect(fill = "pink"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "감정 단어 유형", y = "빈도수 (n)", title = "TOPIC 5에서 사용된 감정 단어 유형")+
  theme(text = element_text(size=40))


# 비율 누적 막대그래프 만들기 - 감정비율 표현
# 더미 변수 생성
freq_score5$dummy <- 0

ggplot(freq_score5, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")), # % 표시
            position = position_stack(vjust = 0.5), # 가운데에 텍스트 표시
            size=20) +
  theme(axis.title.x = element_blank(), # x축 이름 삭제
        axis.text.x = element_blank(), # x축 값 삭제
        axis.ticks.x = element_blank())+ # x축 값 삭제
  theme(legend.box.background = element_rect(fill = "skyblue"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(y = "비율 (총 100%)", title = "TOPIC 5에서 사용된 감정 단어 유형-비율")+
  theme(text = element_text(size=40))

```



## 5-3) 토픽5의 감정 범주 (긍정 /부정) 별 많이 사용되는 단어


```{r}
# score_comment를 토큰화 진행
comment5 <- score_comment5 %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words", # extractNoun이라고 지정할 경우 명사만 나옴. 동사도 나와야하므로 그냥 word를 해줌
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") & # 한글 추출
           str_count(word) >= 2) # 두 글자 이상 추출
head(comment5)


# 감정 및 단어별 빈도 구하기 
freq_word5 <- comment5 %>%
  filter(str_count(word) >= 2) %>% # 2번 이상 등장하는 단어만
  count(sentiment, word, sort = T) %>% print()


# 긍정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word5 %>%
  filter(sentiment == "pos")

# 부정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word5 %>%
  filter(sentiment == "neg")

# 로그오즈비 계산하기
comment_wide1 <- freq_word5 %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment, # sentiment의 범주를 변수로 사용
              values_from = n,
              values_fill = list(n = 0)) %>% print() # 단어 없으면 0

# 로그 오즈비 구하는 공식
comment_wide1 <- comment_wide1 %>%
  mutate(log_RR = log(((pos + 1) / (sum(pos + 1))) / # pos에서의 단어의 비중
                        ((neg + 1) / (sum(neg + 1))))) # neg에서의 단어의 비중 => 에 로그 취한값
comment_wide1

# 로그오즈비가 가장 큰 단어 10개 추출 
top10_topic5 <- comment_wide1 %>%
  group_by(sentiment = ifelse(log_RR > 0, "pos", "neg")) %>%
  slice_max(abs(log_RR), n = 10, with_ties = F) # 로그RR 동점 단어 제외
top10_topic5

#막대 그래프 그리기

ggplot(top10_topic5, aes(x = reorder(word, log_RR), y = log_RR, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(size=10))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "", y = "y는 로그-오즈비 (logRR)", title = "긍정/부정 리뷰에서 자주 사용된 단어", subtitle="TOPIC 5")+
  theme(text = element_text(size=25))
```


## 6-1) 토픽6의 최빈 단어별 감정분석 


```{r}
# 토픽6 선택
topic6<-word_baemin %>% 
  filter(topic==6) %>% 
  select(topic, id, word, reply ) %>% print()

# 감정점수 부여
topic6 <- topic6 %>%
  left_join(senti_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) # 사전에 없는 단어의 점수는 0

topic6 %>% 
  dplyr::select(word, polarity)

# 감정분류
topic6 <- topic6 %>%
  mutate(sentiment = ifelse(polarity ==2, "pos",
                            ifelse(polarity == -2, "neg", "neu")))

topic6 %>%
  count(sentiment)

# 긍정/부정 상위 10개 단어보기
top10_sentiment6 <- topic6 %>%
  filter(sentiment != "neu") %>% # 중립 단어 제외
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% print(n=Inf) # 중복 허용


# 막대 그래프 그리기
top10_sentiment6 %>% 
  ggplot(aes(x = reorder(word, n),
             y = n,
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1,
            color = "dark grey", size=6) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.25))) +
  labs(x = NULL,y = "빈도수 (n)", title = "TOPIC 6에서 자주 사용된 감정단어 ") +
  theme(text = element_text(size=25))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))

```

## 6-2) 토픽6의 전체적인 감정 경향 구하기



```{r}
# 리뷰별 감정 점수 
score_comment6 <- topic6 %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

# 긍정 점수가 높은 리뷰보기 
score_comment6 %>%
  select(score, reply) %>% 
  arrange(-score)

# 부정 점수가 높은 리뷰보기
score_comment6 %>%
  select(score, reply) %>% 
  arrange(score)

# 감정 점수 빈도 
# 전체적으로 0점이 부여된 리뷰이 가장 많고, 긍정과 부정의 양 극단으로 갈 수록 빈도가 감소함
score_comment6 %>%
  count(score)

# 감정 분류하기
score_comment6 <- score_comment6 %>%
  mutate(sentiment = ifelse(score >= 1, "pos",
                            ifelse(score <= -1, "neg", "neu")))

# 감정 빈도 및 비율 구하기
freq_score6 <- score_comment6 %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100) %>% print()

# 막대 그래프 만들기
ggplot(freq_score6, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3, size=20) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))+ # 축 순서 정하기
  theme(legend.box.background = element_rect(fill = "pink"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "감정 단어 유형", y = "빈도수 (n)", title = "TOPIC 6에서 사용된 감정 단어 유형")+
  theme(text = element_text(size=40))


# 비율 누적 막대그래프 만들기 - 감정비율 표현
# 더미 변수 생성
freq_score6$dummy <- 0

ggplot(freq_score6, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")), # % 표시
            position = position_stack(vjust = 0.5), # 가운데에 텍스트 표시
            size=20) +
  theme(axis.title.x = element_blank(), # x축 이름 삭제
        axis.text.x = element_blank(), # x축 값 삭제
        axis.ticks.x = element_blank())+ # x축 값 삭제
  theme(legend.box.background = element_rect(fill = "skyblue"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(y = "비율 (총 100%)", title = "TOPIC 6에서 사용된 감정 단어 유형-비율")+
  theme(text = element_text(size=40))

```



## 6-3) 토픽6의 감정 범주 (긍정 /부정) 별 많이 사용되는 단어


```{r}
# score_comment를 토큰화 진행
comment6 <- score_comment6 %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words", # extractNoun이라고 지정할 경우 명사만 나옴. 동사도 나와야하므로 그냥 word를 해줌
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") & # 한글 추출
           str_count(word) >= 2) # 두 글자 이상 추출
head(comment6)


# 감정 및 단어별 빈도 구하기 
freq_word6 <- comment6 %>%
  filter(str_count(word) >= 2) %>% # 2번 이상 등장하는 단어만
  count(sentiment, word, sort = T) %>% print()


# 긍정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word6 %>%
  filter(sentiment == "pos")

# 부정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word6 %>%
  filter(sentiment == "neg")

# 로그오즈비 계산하기
comment_wide1 <- freq_word6 %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment, # sentiment의 범주를 변수로 사용
              values_from = n,
              values_fill = list(n = 0)) %>% print() # 단어 없으면 0

# 로그 오즈비 구하는 공식
comment_wide1 <- comment_wide1 %>%
  mutate(log_RR = log(((pos + 1) / (sum(pos + 1))) / # pos에서의 단어의 비중
                        ((neg + 1) / (sum(neg + 1))))) # neg에서의 단어의 비중 => 에 로그 취한값
comment_wide1

# 로그오즈비가 가장 큰 단어 10개 추출 
top10_topic6 <- comment_wide1 %>%
  group_by(sentiment = ifelse(log_RR > 0, "pos", "neg")) %>%
  slice_max(abs(log_RR), n = 10, with_ties = F) # 로그RR 동점 단어 제외
top10_topic6

#막대 그래프 그리기

ggplot(top10_topic6, aes(x = reorder(word, log_RR), y = log_RR, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(size=10))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "", y = "y는 로그-오즈비 (logRR)", title = "긍정/부정 리뷰에서 자주 사용된 단어", subtitle="TOPIC 6")+
  theme(text = element_text(size=25))
```

## 7-1) 토픽7의 최빈 단어별 감정분석 


```{r}
# 토픽7 선택
topic7<-word_baemin %>% 
  filter(topic==7) %>% 
  select(topic, id, word, reply ) %>% print()

# 감정점수 부여
topic7 <- topic7 %>%
  left_join(senti_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) # 사전에 없는 단어의 점수는 0

topic7 %>% 
  dplyr::select(word, polarity)

# 감정분류
topic7 <- topic7 %>%
  mutate(sentiment = ifelse(polarity ==2, "pos",
                            ifelse(polarity == -2, "neg", "neu")))

topic7 %>%
  count(sentiment)

# 긍정/부정 상위 10개 단어보기
top10_sentiment7 <- topic7 %>%
  filter(sentiment != "neu") %>% # 중립 단어 제외
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% print(n=Inf) # 중복 허용


# 막대 그래프 그리기
top10_sentiment7 %>% 
  ggplot(aes(x = reorder(word, n),
             y = n,
             fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1,
            color = "dark grey", size=6) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.25))) +
  labs(x = NULL,y = "빈도수 (n)", title = "TOPIC 7에서 자주 사용된 감정단어 ") +
  theme(text = element_text(size=25))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))

```

## 7-2) 토픽7의 전체적인 감정 경향 구하기



```{r}
# 리뷰별 감정 점수 
score_comment7 <- topic7 %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

# 긍정 점수가 높은 리뷰보기 
score_comment7 %>%
  select(score, reply) %>% 
  arrange(-score)

# 부정 점수가 높은 리뷰보기
score_comment7 %>%
  select(score, reply) %>% 
  arrange(score)

# 감정 점수 빈도 
# 전체적으로 0점이 부여된 리뷰이 가장 많고, 긍정과 부정의 양 극단으로 갈 수록 빈도가 감소함
score_comment7 %>%
  count(score)

# 감정 분류하기
score_comment7 <- score_comment7 %>%
  mutate(sentiment = ifelse(score >= 1, "pos",
                            ifelse(score <= -1, "neg", "neu")))

# 감정 빈도 및 비율 구하기
freq_score7 <- score_comment7 %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100) %>% print()

# 막대 그래프 만들기
ggplot(freq_score7, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3, size=20) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))+ # 축 순서 정하기
  theme(legend.box.background = element_rect(fill = "pink"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "감정 단어 유형", y = "빈도수 (n)", title = "TOPIC 7에서 사용된 감정 단어 유형")+
  theme(text = element_text(size=40))


# 비율 누적 막대그래프 만들기 - 감정비율 표현
# 더미 변수 생성
freq_score7$dummy <- 0

ggplot(freq_score7, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")), # % 표시
            position = position_stack(vjust = 0.5), # 가운데에 텍스트 표시
            size=20) +
  theme(axis.title.x = element_blank(), # x축 이름 삭제
        axis.text.x = element_blank(), # x축 값 삭제
        axis.ticks.x = element_blank())+ # x축 값 삭제
  theme(legend.box.background = element_rect(fill = "skyblue"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(y = "비율 (총 100%)", title = "TOPIC 7에서 사용된 감정 단어 유형-비율")+
  theme(text = element_text(size=40))

```



## 7-3) 토픽7의 감정 범주 (긍정 /부정) 별 많이 사용되는 단어


```{r}
# score_comment를 토큰화 진행
comment7 <- score_comment7 %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words", # extractNoun이라고 지정할 경우 명사만 나옴. 동사도 나와야하므로 그냥 word를 해줌
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") & # 한글 추출
           str_count(word) >= 2) # 두 글자 이상 추출
head(comment7)


# 감정 및 단어별 빈도 구하기 
freq_word7 <- comment7 %>%
  filter(str_count(word) >= 2) %>% # 2번 이상 등장하는 단어만
  count(sentiment, word, sort = T) %>% print()


# 긍정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word7 %>%
  filter(sentiment == "pos")

# 부정으로 분리된 리뷰 중 빈번하게 등장하는 단어
freq_word7 %>%
  filter(sentiment == "neg")

# 로그오즈비 계산하기
comment_wide1 <- freq_word7 %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment, # sentiment의 범주를 변수로 사용
              values_from = n,
              values_fill = list(n = 0)) %>% print() # 단어 없으면 0

# 로그 오즈비 구하는 공식
comment_wide1 <- comment_wide1 %>%
  mutate(log_RR = log(((pos + 1) / (sum(pos + 1))) / # pos에서의 단어의 비중
                        ((neg + 1) / (sum(neg + 1))))) # neg에서의 단어의 비중 => 에 로그 취한값
comment_wide1

# 로그오즈비가 가장 큰 단어 10개 추출 
top10_topic7 <- comment_wide1 %>%
  group_by(sentiment = ifelse(log_RR > 0, "pos", "neg")) %>%
  slice_max(abs(log_RR), n = 10, with_ties = F) # 로그RR 동점 단어 제외
top10_topic7

#막대 그래프 그리기

ggplot(top10_topic7, aes(x = reorder(word, log_RR), y = log_RR, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(size=10))+
  theme(legend.box.background = element_rect(fill = "gray"), 
        legend.box.margin = margin(3, 3, 3, 3))+
  labs(x = "", y = "y는 로그-오즈비 (logRR)", title = "긍정/부정 리뷰에서 자주 사용된 단어", subtitle="TOPIC 7")+
  theme(text = element_text(size=25))
```

##### 여기까지,  모든 토픽의 감정분석을 완료하였다. <br/><br/><br/>

#### => 공통적으로 모든 토픽에서 '중립'단어의 비중이 가장 높은 것으로 확인되었다. 최소 65%~80% 정도가 중립 단어이다.

#### => 토픽 1은 긍정 단어 사용 비율이 전체의 16% 정도로 부정 단어 사용 비율보다 높은 편이다. 주로 긍정 문장에서는 '편리성' 에 관한 단어가 도출된다.

#### => 토픽 2는 부정 단어 사용 비율이 전체의 15% 정도로 긍정 단어 사용 비율보다 높은 편이다. 부정 문장에서는 주로 '힘든', '어려운', '시국' 등 COVID-19로 인해 경제적 타격을 맞은 현 상황에 대한 단어가 도출된 것으로 확인된다.

#### => 토픽 3은 부정 단어 사용 비율이 14% 정도로 긍정 단어 사용 비율보다는 조금 더 많은 편이지만 토픽1, 토픽2에 비해 비중이 엇비슷하다. 긍정 문장에서는 토픽 1에서 주로 언급된 '편리성' 에 관한 단어가, 부정 문장에서는 토픽 2에서 주로 나타난 현 어려운 상황에 대한 단어가 도출된 것으로 확인된다. 

#### => 토픽 4는 긍정 단어 사용 비율이 거의 20% 정도이며, 앞 토픽들에 비해 중립 단어 사용은 줄고 긍정 단어 사용이 늘어난 것으로 확인된다. 긍정 문장에서는 주로 '편리성' 혹은 '메뉴' 와 같은 단어가 확인되었다. 

#### => 토픽 5는 중립 단어 사용 비율이 전체의 약 80% 정도로, 중립 단어가 긍정, 부정 단어에 비해 압도적으로 많이 사용된 것으로 확인된다. 심지어 긍정 단어 사용은 9% 정도에 불과하다. 부정 문장에서 주로 사용된 단어로는 '고객센터', '취소', '수수료' 등 고객 서비스에 관한 부분이 많은 것으로 확인된다.

#### => 토픽 6은 긍정 단어 사용이 20% 정도로 가장 많았으며, 중립 단어 사용은 70% 정도, 부정 단어 사용은 가장 적은 것으로 확인되었다. 긍정 문장에서 주로 사용된 단어로는 '할인', '이벤트' 등으로 확인된다. 

#### => 토픽7은 전체 80% 정도가 중립 단어가 사용되었다. 긍정 문장과 부정 문장 전체적으로 '아이디', '앱' 등 어플리케이션 사용성에 관한 단어가 주로 도출되었다. <br/><br/><br/>

# 8. 각 토픽들에 대해 가중로그오즈비에 의한 주요단어 10개들에 다한 막대그래프를 그리시오. (10)

##### 가중 로그 오즈비란? 

##### : 로그 오즈비는 문서 2종에 대한 승산으로 비를 구하므로, 3종 이상의 문서로 구성된 말뭉치에 적용할 수 없는 한계가 있다.

#####  tf_idf는 3종 이상의 문서로 구성된 말뭉치에 적용할 수 있지만, tf_idf계산 방식에서 오는 한계가 있다.

##### 따라서 이것의 대안으로 로그 오즈비에 베이지언 확률모형을 적용한 것이 가중 로그 오즈비이다.

##### 가중 로그 오즈비를 구할 경우 상대적인 로그 오즈 (승산), 즉 상대적으로 많이 쓰이는 단어를 알 수 있다. <br/><br/>


###### 4번에서  tf-idf 값까지 구했던 freq를 사용하여 가중 로그비를 구한다.

```{r}

# 가중 로그 오즈비 구하기 (앞에서 사용한 freq 사용)
freq_log <- freq %>% 
  bind_log_odds(set = topic, feature = word, n = n) %>%
  arrange(desc(log_odds_weighted))
freq_log

# 주요 10개 단어 추출
top10_log <- freq_log %>%
  group_by(topic) %>%
  slice_max(log_odds_weighted, n = 10, with_ties = F)


# 그래프 순서 지정
top10_log$topic <- factor(top10_log$topic,
                         levels = c(1:7))

# 그래프 그리기
top10_log %>% 
  ggplot(aes(x = reorder_within(x=word, by=log_odds_weighted, within=topic),
             y = log_odds_weighted,
             fill = topic)) +
  labs(x = "", y = "y는 log_odds_weighted", title = "TOPIC별 중요한 단어", subtitle="(타 TOPIC 대비 상대적으로 많이 나오는 단어)") +
  geom_text(aes(label = round(log_odds_weighted,4) , hjust = -0.1),
            color = "dark grey", size=6)+ 
  geom_col(show.legend = F) +
  coord_flip() +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  scale_x_reordered() +
  labs(x = NULL) +
  theme(text = element_text(size=25))

```

### => 



# 9. 위의 모든 것을 고려하여 각 토픽에 대한 이름을 짓고 설명하시오. (10)


###### 토픽별 이름을 짓는 것은 토픽별 전체적인 리뷰의 내용을 파악한 후 축약하는, 연구자의 주관이 들어간 분야이다. 

###### 토픽 모델링은 다수의 문서에서 잠재적으로 의미있는 토픽을 발견하는 것으로 다량의 텍스트의 핵심 주제를 찾아 비슷한 내용으로 분류하는 방법이다.

###### 따라서 분리된 토픽의 주제가 무엇인지 파악해서 이름을 짓는 것이야 말로 광범위한 다량의 비정형 텍스트 데이터에서 인사이트를 발견할 수 있는 방법이다.



###### beta를 사용해서 만든 term_topic (단어가 들어있음) 을 통해 토픽 별 beta 값이 높은 단어 6개를 추려낸다.

```{r}

# 토픽별 문서 수와 단어를 막대그래프로 나타내기

# 토픽별 주요 단어 목록 만들기 (토픽별 beta 값이 높은 상위 6개의 단어가 나옴)
top6_terms <- term_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 6, with_ties = F) %>%
  summarise(term = paste(term, collapse = ", "))
top6_terms


#토픽별 문서 빈도 구하기
count_topic <- baemin_topic %>%
  count(topic) %>% print()

# 문서 빈도에 주요 단어 결합
count_topic_word <- count_topic %>%
  left_join(top6_terms, by = "topic") %>%
  mutate(topic_name = paste("Topic", topic)) %>% print()


# 토픽 별 문서 수와 주요 단어를 막대 그래프로 나타내기
ggplot(count_topic_word,
       aes(x = reorder(topic_name, n),
           y = n,
           fill = topic_name)) +
  geom_col(show.legend = F) +
  coord_flip() +
  geom_text(aes(label = comma(n, accuracy = 1)),  # 문서 빈도 표시
            hjust = -0.2, 
            col = "dark grey",
            size=12) + # 막대 밖에 표시
  geom_text(aes(label = term), # 주요 단어 표시
            hjust = 1.03, # 이건 막대 안에 표시
            col = "white",
            size=12) + 
  scale_y_continuous(expand = c(0, 0),  # y축-막대 간격 줄이기
                     limits = c(0, 700)) +
  labs(title = "토픽별 주요 단어 및 리뷰 빈도",
       x = NULL, y = NULL) +
  theme_minimal() +
  theme(plot.title = element_text(size = 30, face = "bold"),
        text = element_text(size=25))

```

###### 이 과정을 통해 토픽 별 문서 수와 주요 단어를 한 눈에 파악할 수 있다.


##### 다음으로 gamma를 활용해 토픽별로 분리가 된 원문 (리뷰) 데이터인 baemin_topic을 활용한다.

###### gamma가 높은 순으로 나오게 하며, 토픽별로 gamma가 높은 상위 50개의 문장만 나오게한다.

```{r}

# 토픽별로 gamma가 높은 50개의 주요 문서 (리뷰) 추출
reply_topic <- baemin_topic %>%
  group_by(topic) %>%
  slice_max(gamma, n = 50) %>% 
  select(topic, gamma, reply) %>% 
  arrange(topic,-gamma) %>% print() #gamma가 높은 순으로 나옴

```

#### 이제부터 토픽별로 (총 7개) 작성된 리뷰의 내용을 살펴본다.

```{r}
# 토픽1 내용 살펴보기
reply_topic %>%
  filter(topic == 1) %>%
  pull(reply)
```

#### => 첫 번째 토픽은 '배달의 민족' 의 가장 기본적이고 근간이 되는 서비스인 '배달 서비스'이다. 

#### 토픽 내 리뷰들에서 '배달', '라이더', '주소' 등의 단어가 사용된 것이 확인되었으며, 특히 '배달'이라는 단어는 상당히 많은 수의 리뷰에 사용되었다.

#### 어플 이용시의 불편 및 서비스 개선 사항 보다는 라이더가 제품을 픽업한 후 고객에게 전달하는 것을 뜻하는 배달 (라이더) 서비스와 배달 불가 지역에 대한 의견이 주로 도출된 것이 확인된다.

#### '배달의 민족' 어플리케이션 서비스 개선도 필요하지만 가장 기본적인 딜리버리 (주소 전달 - 라이더 픽업- 고객 전달) 서비스의 개선 역시 필요하다는 것을 알 수 있다.

```{r}
# 토픽2 내용 살펴보기
reply_topic %>%
  filter(topic == 2) %>%
  pull(reply)
```

#### => 두번째 토픽은 '기업 윤리'이다. 

#### 특히 대부분의 리뷰에 '수수료',독과점' 등의 단어와 함께 '삭제','탈퇴','실망' 등의 극단적인 부정적 감정을 표출하는 단어가 같이 사용되었다. 

#### 특히, ‘수수료’ 키워드가 가장 높은 가중치를 기록하며 토픽 내 상당수의 리뷰에 등장한 것으로 확인되었다. 

#### 이 토픽은 2020년 4월 배달의 민족을 운영하는 ‘우아한 형제들’이 배달 매출의 5.8%를 수수료로 떼는 정률제를 도입하는 이슈와 관련되어 발생하였다. 

#### 또한 독일 기업 딜리버리히어로가 ‘배달의 민족’, ‘요기요’, ‘배달통’을 모두 소유하며 시장의 98%를 점유하게 된 상황에서 자영업자에게 부담을 주는 수수료 정책은 사회적으로 반감을 일으켰고 불매운동이 이어지자, 배달의 민족은 공식 사과와 함께 수습에 나섰다.

#### 이는 서비스를 제공하는 기업의 정책이 기업 윤리를 위반한다고 판단될 때 기업에 치명적인 영향을 줄 수 있음을 보여주고 있다. 

```{r}
# 토픽3 내용 살펴보기 : 
reply_topic %>%
  filter(topic == 3) %>%
  pull(reply)
```
#### => 세번째 토픽은 '독과점으로 인한 고객이탈' 이다.

#### 해당 토픽에서는 앱 이용 불만, 할인 혜택 불만 등 다양한 사례로 고객 이탈하겠다는 고객들의 의견이 도출되었다. 

#### 그런데 고객들이 주장하는 가장 큰 이탈 사유는 배달의 민족의 '독과점' 때문이라는 것이다. 

#### '배달의 민족', '요기요', '배달통' 등 배달 어플리케이션 3사는 치열하게 다투고 있다. 고객들은 이러한 어느 한 기업의 힘이 강해졌다고 생각할 경우 이를 독과점으로 인한 횡포로 생각해 해당 기업에 반감을 가질 수 있고 이는 곧 고객 이탈을 야기할 수 있다는 것을 보여주고 있다.


```{r}
# 토픽4 내용 살펴보기
reply_topic %>%
  filter(topic == 4) %>%
  pull(reply)
```

#### 네번째 토픽은 ‘고객 리뷰’이다.

#### 해당 토픽내 각 리뷰에서 ‘리뷰’, ’음식’, ’사진’, ’가게’ 등의 단어가 추출되었으며 '리뷰'라는 각 리뷰에 상당수가 들어있는 것으로 확인되었다.

#### 어플내 리뷰 작성 과정에서 발생한 불편 혹은 신뢰성 없는 리뷰로 인한 불만이 주 내용이다. 

#### 구매자는 리뷰를 작성함으로서 공급자에게 의견을 전달하며, 또한 다른 구매자의 리뷰를 읽음으로써 구매 의사결정에 필요한 정보를 획득한다. 

#### 온라인 쇼핑몰을 대상으로 수행한 연구에서는 온라인 리뷰를 통해 제품에 대한 태도와 경험을 공유 (리뷰 작성의 형태로 나타남) 하여 구매 의도와 재방문 의도에 영향력이 미친다는 것을 확인할 수 있다.

```{r}
# 토픽5 내용 살펴보기
reply_topic %>%
  filter(topic == 5) %>%
  pull(reply)
```


#### => 다섯번째 토픽은  ‘음식점 서비스'이다. 

#### 해당 토픽 내 리뷰에서 '주문', '전화’, ‘시간’, ‘취소’, ‘서비스’ 등의 단어가 주로 들어간다. 

#### 또한 배달 지연, 배달 주문시간 오설정, 주문 취소 불가 관련 등의 불만이 주 내용인데 이러한 불편사항은 '배달의 민족' 자체의 문제 (ex. APP 관련) 라기 보다는 음식점 서비스 과련 불만으로 분류해야 할 것이다. 

#### '배달의 민족'은 플랫폼 서비스로, 서비스상에서 소비자인 일반 이용자와 공급자인 음식점(업체)의 거래가 이루어지는 곳이다. 

#### 따라서 앞에서 언급한 내용들은 매장에서 결정, 진행, 적용하는 사항들이다.

#### 해당 토픽을 통해 외식배달 플랫폼 서비스 공급자 (음식점) 에 대한 신뢰는 플랫폼 서비스 자체 대한 만족도에 긍정적인 영향을 미친다는 것이 확인된다. 

#### 따라서, 플랫폼 서비스를 제공하는 배달의 민족 사업자뿐만 아니라 공급자인 음식점(업체)가 제공하는 가격, 품질, 서비스가 플랫폼 서비스 고객 만족도에 영향을 미칠 수 있음을 보여준다.

#### '배달의 민족'은 음식점의 서비스를 배재할 수 없기에 서비스 모니터링을 통해 고객과 공급자인 음식점 업주 간의 거래를 원활하게 하기 위한 노력을 기울일 필요가 있는 것이다.


```{r}
# 토픽6 내용 살펴보기
reply_topic %>%
  filter(topic == 6) %>%
  pull(reply)
```

#### => 여섯번째 토픽은 ‘이벤트 기간 내 발생한 특정 기기의 어플리케이션 오류’이다.

#### 해당 토픽에 대해 상세하게 표현한 이유는 어플리케이션 이용 중 발생하는 오류라는 점에서 토픽 7과 유사한 측면이 있기 때문이다. 따라서 토픽 7과 구분하기 위해 상세하게 기재하였다.

#### 해당 토픽은 시계열 분석한 결과 특정일에 집중적으로 등록된 리뷰라는 것이 확인된다. 

#### 확인 결과 실제 이벤트 기간에 안드로이드 (특정 기기) 기기에서만 최종 배달 금액이 뜨지 않는 등 이벤트 주문 접수가 불가한 문제가 발생한 것으로 파악된다. 

#### 추가로 이벤트은 일정 기간동안 진행되었지만 해당 토픽의 리뷰들은 상당 수가 이벤트 종료 당일과 종료 바로 다음 날에 기입되었다. 

#### 만약 해당 문제가 이벤트 기간 초반에 발생하였을 때와 현재 토픽의 리뷰들처럼 이벤트 종료 직전에 발생했을 때를 비교해서 작성 수 (주문량은 동일하다 가정) 나 감정을 분석한다면 고객 분석 (마케팅이나 CS) 의 측면에서 도움이 될 수도 있으리라 판단된다.

#### 따라서 이 날 발생한 유사한 현상에 대해 많은 고객들이 다른 언어적 표현을 사용하면서 리뷰를 남겼다. 하지만 토픽 모델링 결과 해당 고객들 대부분이 토픽 5 에 들어간 것으로 확인된다.




```{r}
# 토픽7 내용 살펴보기 
reply_topic %>%
  filter(topic == 7) %>%
  pull(reply)
```

#### => 일곱번째 토픽은 '어플리케이션 오류' 이다.

#### 로그인, 카드 등록, 결제 등 유저가 어플리케이션을 설치한 후 주문 접수하기까지에서 발생하는 어플리케이션 오류 상황에 대한 내용이다. 

#### 앞서 언급한 것과 같이 주로 인증 오류의 형태가 많은 것으로 파악된다.

#### 다섯번째 토픽이 특정한 일자에 집중적으로 발생했던 오류라면, 여섯번째 토픽은 유저 개인에게 있어 항시 발생할 수 있는 오류에 대한 내용이다.

#### 또한 여러 가지 오류와 더불어 서비스 업체 (배달의 민족 고객센터)의 대응과 관련된 토픽이다. 

#### 2019년 한 해 동안 소비자고발센터에 제기된 배달 애플리케이션 배달의 민족, 요기요, 배달통에 대한 소비자 민원을 집계한 결과 가장 많은 민원을 차지한 것은 시스템 오류(36.6%)와 관련된 것이었다.

#### 서비스 오류와 관련하여 소비자 고발센터에 가장 많은 민원이 제기되었다는 점에서 서비스의 오류는 단순히 이용률 감소로 이어지는 것이 아니라, 애플리케이션 자체에 대한 불신과 이탈로 이어질 수 있음을 보여주고 있다


##### 각 토픽에 대해 파악했다면 알맞은 이름을 붙여준다.


```{r}

# 토픽 이름 목록 만들기
name_topic <- tibble(topic = 1:7,
                     name = c("1. 기본적인 배달 서비스 \n개선 필요",
                              "2. 수수료 정책은 \n불매 운동을 불러와",
                              "3. 배달시장 독식은 \n고객 이탈을 불러와",
                              "4. 작성 오류 없고 믿을만한 \n리뷰 작성이 진행되어야",
                              "5. 주문 접수부터 취소까지, \n음식점 자체 서비스 개선 필요",
                              "6. 이벤트를 열어도 안드로이드 기기에서만  \n주문 오류 발생?!",
                              "7. 로그인부터 결제까지, \n다양한 APP 사용 오류 속 대응도 엉망"))


# 토픽 이름 결합하기
top_term_topic_name <- top10_term_topic %>%
  left_join(name_topic,  by = "topic")
top_term_topic_name

# 막대 그래프 만들기
ggplot(top_term_topic_name,
       aes(x = reorder_within(term, beta, name),
           y = beta,
           fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~ name, scales = "free", ncol = 3) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "배달의 민족 고객 리뷰 토픽",
       subtitle = "토픽별 주요 단어 TOP 10",
       x = NULL, y = NULL) +
  theme_minimal() +
  theme(text = element_text(size=20),
        plot.title = element_text(size = 30, face = "bold"),
        plot.subtitle = element_text(size = 25),
        axis.text.x = element_blank(),   # x축 이름 삭제
        axis.ticks.x = element_blank())  # x축 눈금 삭제
```

##### => 이렇게 토픽 모델링은 종료된다.



# 10. 전체적으로 배달의 민족의 리뷰에 대한 텍스트마이닝을 통해 제시하고자 하는 시사점을 설명하시오. (10)

###### 각 토픽별 시사점은 9번 항목에서 이미 세세하게 정의하였다.


#### => 이번 '배달의 민족 - 텍스트 마이닝' 프로젝트는 빅데이터 시대에 걸맞는 분석 방법이라 할 수 있다.
#### 소비자의 행태를 파악하기 위하여 사용자 리뷰라는 비정형 텍스트 데이터를 분석하고 그 결과에서 통찰을 찾아낸 것이다.

#### 즉, 해당 연구는 과거 조사방법론 중심의 전자상거래 관련 연구와 차별화된다. 인위적이고 조작되지 않은 (여기서 말하는 조작은 조작화 과정을 뜻한다.) 상황에서 가공되지 않은 실제적 데이터를 분석하는 과정에서 보다 고객의 솔직한 답변을 기대할 수 있으며 분석의 근간의 되는 데이터 수집 역시 빠르게 진행할 수 있는 것이다. <br/><br/>

#### => 또한 사용자 리뷰 데이터를 바탕으로 LDA 토픽모델링, 감성 분석을 활용한 고객 리뷰 분석을 통해 각 토픽 (문제상황별) 이슈 대응 프로세스를 정립하였다. (9번 각 토픽 항목에 기재함)  <br/><br/>

#### => 추가로 이번 프로젝트에서는 리뷰가 작성 날짜에 대해서는 분석하지 않았지만 현재 원문에 주어진 것처럼 리뷰 작성 일자 및 시간을 포함하여 시계열 분석을 진행한다면 보다 좋은 분석 결과를 얻을 수 있다.


###### 예를 들어 설명해보도록 하겠다.

```{r}
# 간단한 시계열 분석을 통해 토픽별 리뷰가 등록된 날짜와 등록된 리뷰수를 파악해줌

ts_topic<-baemin_topic %>% 
  group_by(날짜) %>% 
  count(topic) %>% 
  arrange(-n)

ggplot(ts_topic, aes(x=날짜, y=n, colour=factor(topic)))+
  geom_line(size=1.0) +
  ggtitle("토픽별 리뷰 등록 날짜") +
  labs(x = "등록 날짜", y = "등록 리뷰수 (n)")+
  theme(plot.title=element_text(size=25))
```

#### => 해당 그래프는 토픽별 리뷰 등록 날짜와 해당 날짜에 등록된 리뷰수에 관한 그래프이다. 

#### 이 그래프에서 나타나듯이 토픽별로 매우 상이한 결과가 나오는 것이 확인된다.

### 따라서, 날짜별 어떤 토픽의 리뷰들이 작성되었는지, 특정한 토픽의 리뷰가 집중되서 등록되었는지 아닌지, 작성된 리뷰의 수와 토픽 내 리뷰의 감정 분석 진행시 어떤 결과를 도출해낼 수 있는지 등을 파악한다면 특정 서비스 이슈에 대해 빠르게 감지, 대응가능한 정책 혹은 MIS (자동화된 형태) 를 구축할 수 있을 것으로 판단된다. 



